sac:
  alpha: 0.1 # 初始alpha值，float类型
  alr: 1e-4 # 用于更新alpha的学习率，float类型
  qlr: 1e-4 # 用于更新q函数的学习率，float类型
  policy_lr: 1e-4 # 用于更新策略函数的学习率，float类型
  mem_size: 1e6 # 回放缓冲区的大小，int类型，默认为1e6

training:
  max_total_steps: 500000 # 训练过程中环境步骤的最大值
  max_steps: 1000 # 每个episode的最大步骤数
  batch_size: 256 # 从回放记忆中提取的样本大小
  intermediate_policies: True # 是否保存每个训练阶段的中间策略
  verbose: True # 是否打印训练过程中的详细信息
  path: "./" # 保存中间策略的路径，默认为'./'
  final_policy: "./final_policy.pth" # 保存中间策略的路径，默认为'./'
  update_all: True # 是否在每个环境步骤后更新。默认为True

# The gym environment for the mdoger7.
environment:
  action_repeat: 1 # 运动重复的次数
  distance_weight: 10.0 # 距离项在奖励中的权重
  energy_weight: 0.5 # 能量项在奖励中的权重
  shake_weight: 5.0 # 垂直摇晃项在奖励中的权重
  drift_weight: 5.0 # 侧向漂移项在奖励中的权重
  distance_limit: .inf #float("inf") # 终止episode的最大距离
  observation_noise_stdev: 0.0 # 观察噪声的标准差
  self_collision_enabled: True # 是否允许机器人自身碰撞
  motor_velocity_limit: .inf #np.inf # 每个马达的速度限制
  pd_control_enabled: False # 是否为每个马达启用PD控制器
  leg_model_enabled: True # 是否使用腿部马达重新参数化动作空间
  accurate_motor_model_enabled: True # 是否使用准确的直流电机模型
  motor_kp: 2.0 # 准确电机模型的比例增益
  motor_kd: 0.03 # 准确电机模型的微分增益
  torque_control_enabled: False # 是否使用扭矩控制，如果设置为False，则使用姿态控制
  motor_overheat_protection: False # 是否关闭已施加大力矩(OVERHEAT_SHUTDOWN_TORQUE)的电机，以防止过热(OVERHEAT_SHUTDOWN_TIME)。有关更多详细信息，请参见minitaur.py中的ApplyAction()函数。
  hard_reset: True # 是否在重置时清除仿真并加载所有内容。如果设置为false，则重置只是将小牛放回起始位置并将其姿势设为初始配置。
  on_rack: False # 是否将小牛放在架子上。这仅用于调试行走步态。在此模式下，小牛的基座悬挂在半空中，以便更清晰地可视化其步态。
  render: True # 是否渲染仿真
  kd_for_pd_controllers: 0.3 # 用于马达的PD控制器的kd值
